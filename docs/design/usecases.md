---
layout: default
title: Use cases
---

### Identifying studies ###

 - Alone, from scratch
 - As a team, from scratch
 - Using the current *consensus*
    - Structural information
    - Textual queries
    - Both structural + textual?
 - Combinations of the above (how?)
 - Automated methods

### Extracting study data ###

 - Alone, from scratch
 - As a team, from scratch
 - Using the current *consensus* 
 - Consensus + alone
 - Consensus + team
 - Non-consensus opinion
 - Non-consensus opinion + alone
 - Non-consensus opinion + team
 - Using a 3rd-party system (SRDR?)
 - Automated import from structured sources 
 - Automated text extraction 

### Constructing a "consensus" fact base ###

 - A single "consensus" fact base is important for communicating the value of system.
 - The consensus must clearly be supported by statements made by individual contributors.
 - Attempts at "gaming the system" must be detected.
 - Reliability of statements and of users / etc. must be assessed.

### Referring to study data ###

 - To a specific version
    - So consensus X + deltas Y, Z
 - To the most recent version
    - So consensus NOW + deltas

### Mapping concepts between studies ###

### Constructing data sets ###

Q: is this really different from identifying studies? Would this be the starting point for a review which might then trigger a review to find studies in the literature?

Is there only a single data set per project? Or can there be multiple?

 - Direct result of review process?
 - Manual study selection
 - Rule-based study selection
 - Take inclusion of existing project / review / etc.
 - ...?

### Matching studies to analyses ###

Getting into ADDIS territory but needs to be considered from a data model and querying POV.

 - Starting from a specific dataset

### Viewing extracted data ###

### Discussing extractions / contributing to the consensus view ###

 - Making minor corrections?
 - Micro-contributions?

### Investigating provenance ###

Different types of statements:

 - Statements accepted due to specific rules (e.g. "alone" or "team review" scenarios of extraction)
 - Generally accepted statements
 - Disputed statements
 - Statements with weak support
 - Statements just below threshold?

Different types of attribution:

 - Contributions made directly by users
 - Contributions made by users through 3rd party systems (e.g. SRDR)
 - Contributions made on behalf of users by automated agents
 - Contributions made on behalf of organizations / groups by automated agents

Attribution / giving credit as a goal in itself:

 - Allow users to showcase their contributions made
 - Allow referring others directly to a record as extracted by you
 - Show contributions to key projects, citation indexes of finally published reviews, etc.
 - Detect when users have been key in finding mistakes?

### Detecting contradictions ###

 - Topic locality is important for this, to help isolate problems. FIXME: put this in a use case type thing.

### Embargo ###

 - Place contributions made as part of project under embargo, for a price. Protect the work until it has been published. Possible business model.

## Abuse cases ##

### Lying about things ###

### Deploying inappropriate computational agents ###

### What else? ###
